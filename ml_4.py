# -*- coding: utf-8 -*-
"""ML 4

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1lJANdszuMABYBi0RjS0oZ7mWrzPlDt0v
"""

# Importing libraries like pandas, numpy, matplotlib, and seaborn.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

data = pd.read_csv("sales_data_sample.csv",encoding='Latin-1')
data.head()

# Check for number of rows and columns using shape method
data.shape

# Number of NAN values per column in the dataset
data.isnull().sum()

# Drop columns from dataset which are no longer required for the problem statement
data.drop(["ORDERNUMBER", "PRICEEACH", "ORDERDATE", "PHONE", "ADDRESSLINE1", "ADDRESSLINE2", "CITY", "STATE", "TERRITORY", "POSTALCODE", "CONTACTLASTNAME", "CONTACTFIRSTNAME"], axis = 1, inplace=True)

# View first 5 entries of data after dropping columns
data.head()

# Number of NAN values per column in the dataset
data.isnull().sum()

# Generate descriptive analysis of the data
data.describe()

# Count number of data in each category of STATUS column using countplot() in seaborn library
sns.countplot(data = data , x = 'STATUS')

import seaborn as sns

# Convert unique values from PRODUCTLINE column into an array
data['PRODUCTLINE'].unique()

# Checking the duplicated values 
data.drop_duplicates(inplace=True)

# Check datatype for each column
data.info()

# Concatenate object datatype values into a list
list_cat = data.select_dtypes(include=['object']).columns.tolist()

# Display list 
list_cat

# Visualize into countplot for each column in list
for i in list_cat:
  sns.countplot(data = data ,x = i)
  plt.xticks(rotation = 90)
  plt.show()

# Dealing with the categorical features 
# Conversion of other datatypes into numerical form using LabelEncoder
from sklearn import preprocessing
le = preprocessing.LabelEncoder()

# Encode labels in column 'species'.
for i in list_cat:
  data[i]= le.fit_transform(data[i])

data.info()

# Converting float datatype of SALES column into int
data['SALES'] = data['SALES'].astype(int)

data.info()

data.describe()

# Taget feature are SALES and PRODUCTCODE
X = data[['SALES','PRODUCTCODE']]

# Viewing final columns of the data
data.columns

# Importing KMeans method from sklearn library
from sklearn.cluster import KMeans
km=KMeans(1)

# Importing Elbow Visualizer from yellowbrick library and plotting a K Means graph using Elbow Method
from yellowbrick.cluster import KElbowVisualizer
model = KMeans()
visualizer = KElbowVisualizer(model, k=(1,12)).fit(X)
visualizer.show()

# Thus number of clusters = 4. Initialize using k-means++.
from sklearn.cluster import KMeans
kmeans = KMeans(n_clusters=4, init='k-means++', random_state=0).fit(X)

# Print labels for each data point in the dataset
kmeans.labels_

# Final inertia value after fitting a KMeans() by using kmeans.inertia_
'''Inertia is calculated by measuring the distance between each data point and its centroid, squaring this distance, 
    and summing these squares across one cluster. A good model is one with low inertia & a low number of clusters (k).
'''
kmeans.inertia_

# Iterations
kmeans.n_iter_

# Cluster centers for each cluster
kmeans.cluster_centers_

# Getting the size of the clusters 
from collections import Counter
Counter(kmeans.labels_)

#Hence, its verified that the number of Clusters to be choosen will be 4 according to the elbow method.
sns.scatterplot(data=X, x="SALES", y="PRODUCTCODE", hue=kmeans.labels_)
plt.scatter(kmeans.cluster_centers_[:,0], kmeans.cluster_centers_[:,1], 
            marker="X", c="r", s=80, label="centroids")
plt.legend()
plt.show()